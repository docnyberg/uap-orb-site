#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
motif_miner.py
Mine frequent motifs from per-video *.sequence.json generated by build_sequences.py.

Enhancements:
- --rle-collapse: run-length collapse before mining to remove dwell dominance
- --strip-orientation / --strip-flash: ignore ↑/↓ and/or ✦ when desired
- --min-distinct: require >=K distinct tokens in a motif (filters AAA…)
- --forbid-pure-runs: drop motifs that consist of the same token repeated
- --insert-rest-between-cycles / --unknown-as-rest: same as before
- Exports top motifs ordered by frequency

Usage:
  python motif_miner.py ^
    --sequences "C:\...\public\sequences" ^
    --out       "C:\...\public\canonical_motifs.json" ^
    --min-len   3 ^
    --max-len   6 ^
    --min-freq  2 ^
    --insert-rest-between-cycles ^
    --unknown-as-rest ^
    --rle-collapse ^
    --strip-orientation ^
    --min-distinct 2 ^
    --forbid-pure-runs
"""

import argparse, json
from pathlib import Path
from typing import List, Dict, Any, Iterable, Tuple
from collections import defaultdict, Counter

# ----------------- IO -----------------

def load_json(p: Path) -> dict:
    return json.loads(p.read_text(encoding="utf-8"))

def save_json(p: Path, obj: dict) -> None:
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")

# ----------------- Token utilities -----------------

def tokenize(seq: str) -> List[str]:
    """symbol_seq_enhanced is space-separated tokens (e.g., 'Green↑ Blue Pink/Magenta✦ ...')."""
    return [tok for tok in (seq or "").split() if tok]

def strip_orientation(tok: str) -> str:
    # remove single trailing arrow if present (↑/↓)
    if tok.endswith("↑") or tok.endswith("↓"):
        return tok[:-1]
    return tok

def strip_flash(tok: str) -> str:
    # remove trailing ✦ if present
    return tok[:-1] if tok.endswith("✦") else tok

def normalize_token(tok: str, strip_ori: bool, strip_fla: bool) -> str:
    t = tok
    if strip_ori:
        t = strip_orientation(t)
    if strip_fla:
        t = strip_flash(t)
    return t

def rle_collapse(tokens: List[str]) -> List[str]:
    """Run-length collapse: A A A B B C -> A B C"""
    if not tokens:
        return tokens
    out = [tokens[0]]
    for t in tokens[1:]:
        if t != out[-1]:
            out.append(t)
    return out

def motif_is_pure_run(motif: Tuple[str, ...]) -> bool:
    first = motif[0]
    return all(t == first for t in motif)

def motif_distinct_count(motif: Tuple[str, ...]) -> int:
    return len(set(motif))

# ----------------- Sequence building -----------------

def build_tokens_from_obj(obj: dict,
                          insert_rest: bool,
                          unknown_as_rest: bool,
                          strip_ori: bool,
                          strip_fla: bool,
                          rle: bool) -> List[str]:
    """
    Build a token list from a single sequence JSON:
      - base: symbol_seq_enhanced (space-separated)
      - unknown_as_rest: map Unknown* -> rest
      - insert_rest: place 'rest' between cycles (obj['cycles']) if present
      - strip_*: remove ↑/↓ and/or ✦
      - rle: apply run-length collapse at the end
    """
    def norm_list(raw: List[str]) -> List[str]:
        out = []
        for t in raw:
            if unknown_as_rest and t.startswith("Unknown"):
                t2 = "rest"
            else:
                t2 = normalize_token(t, strip_ori=strip_ori, strip_fla=strip_fla)
            out.append(t2)
        return out

    # Preferred: respect cycles when we want rest between them
    if insert_rest:
        cycles = obj.get("cycles", [])
        if cycles and isinstance(cycles, list) and len(cycles) >= 1:
            seq_all = []
            for i, cyc in enumerate(cycles):
                toks = tokenize(cyc)
                seq_all.extend(norm_list(toks))
                if i < len(cycles) - 1:
                    seq_all.append("rest")
        else:
            toks = tokenize(obj.get("symbol_seq_enhanced",""))
            seq_all = norm_list(toks)
    else:
        toks = tokenize(obj.get("symbol_seq_enhanced",""))
        seq_all = norm_list(toks)

    if rle:
        seq_all = rle_collapse(seq_all)
    return seq_all

# ----------------- Mining -----------------

def mine_ngrams(sequences: Iterable[List[str]],
                min_len: int,
                max_len: int,
                min_freq: int,
                min_distinct: int,
                forbid_pure_runs: bool) -> Dict[str, Any]:
    counts = defaultdict(int)
    total_seqs = 0
    for seq in sequences:
        if not seq:
            continue
        total_seqs += 1
        N = len(seq)
        for L in range(min_len, max_len + 1):
            if L > N:
                continue
            for i in range(0, N - L + 1):
                motif = tuple(seq[i:i+L])
                if forbid_pure_runs and motif_is_pure_run(motif):
                    continue
                if motif_distinct_count(motif) < min_distinct:
                    continue
                counts[motif] += 1

    out = {
        "meta": {
            "total_sequences": total_seqs,
            "min_len": min_len, "max_len": max_len, "min_freq": min_freq,
            "min_distinct": min_distinct,
            "forbid_pure_runs": bool(forbid_pure_runs),
        },
        "canonical_motifs": []
    }
    for motif, cnt in counts.items():
        if cnt >= min_freq:
            out["canonical_motifs"].append({"sequence": list(motif), "frequency": int(cnt)})

    out["canonical_motifs"].sort(key=lambda d: (-d["frequency"], d["sequence"]))
    return out

# ----------------- Main -----------------

def main():
    ap = argparse.ArgumentParser(description="Mine frequent motifs across *.sequence.json")
    ap.add_argument("--sequences", required=True, help="Folder with *.sequence.json")
    ap.add_argument("--out", required=True, help="Output JSON for canonical motifs")

    ap.add_argument("--min-len", type=int, default=3)
    ap.add_argument("--max-len", type=int, default=6)
    ap.add_argument("--min-freq", type=int, default=2)

    ap.add_argument("--insert-rest-between-cycles", action="store_true")
    ap.add_argument("--unknown-as-rest", action="store_true")

    # New normalization flags
    ap.add_argument("--rle-collapse", action="store_true", help="Run-length collapse tokens before mining")
    ap.add_argument("--strip-orientation", action="store_true", help="Remove ↑/↓ from tokens")
    ap.add_argument("--strip-flash", action="store_true", help="Remove trailing ✦ from tokens")

    # New filters
    ap.add_argument("--min-distinct", type=int, default=1, help="Require >=K distinct tokens in motif")
    ap.add_argument("--forbid-pure-runs", action="store_true", help="Drop motifs like A,A,A,...")

    args = ap.parse_args()

    seq_dir = Path(args.sequences)
    files = sorted(seq_dir.glob("*.sequence.json"))

    seqs: List[List[str]] = []
    for p in files:
        data = load_json(p)
        toks = build_tokens_from_obj(
            data,
            insert_rest=bool(args.insert_rest_between_cycles),
            unknown_as_rest=bool(args.unknown_as_rest),
            strip_ori=bool(args.strip_orientation),
            strip_fla=bool(args.strip_flash),
            rle=bool(args.rle_collapse)
        )
        seqs.append(toks)

    res = mine_ngrams(
        sequences=seqs,
        min_len=int(args.min_len),
        max_len=int(args.max_len),
        min_freq=int(args.min_freq),
        min_distinct=int(args.min_distinct),
        forbid_pure_runs=bool(args.forbid_pure_runs)
    )

    save_json(Path(args.out), res)
    print(f"[OK] wrote {args.out} with {len(res['canonical_motifs'])} motifs")

if __name__ == "__main__":
    main()
